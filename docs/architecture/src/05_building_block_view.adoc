ifndef::imagesdir[:imagesdir: ../images]

[[section-building-block-view]]


=== Building Block View

////
.Content
The building block view shows the static decomposition of the system into building blocks (modules, components, subsystems, classes, interfaces, packages, libraries, frameworks, layers, partitions, tiers, functions, macros, operations, data structures, ...) as well as their dependencies (relationships, associations, ...)

This view is mandatory for every architecture documentation.
In analogy to a house this is the _floor plan_.

.Motivation
Maintain an overview of your source code by making its structure understandable through
abstraction.

This allows you to communicate with your stakeholder on an abstract level without disclosing implementation details.

.Form
The building block view is a hierarchical collection of black boxes and white boxes
(see figure below) and their descriptions.

image::05_building_blocks-EN.png["Hierarchy of building blocks"]

*Level 1* is the white box description of the overall system together with black
box descriptions of all contained building blocks.

*Level 2* zooms into some building blocks of level 1.
Thus it contains the white box description of selected building blocks of level 1, together with black box descriptions of their internal building blocks.

*Level 3* zooms into selected building blocks of level 2, and so on.


.Further Information

See https://docs.arc42.org/section-5/[Building Block View] in the arc42 documentation.

////


==== Whitebox Overall System

////
Here you describe the decomposition of the overall system using the following white box template. It contains

 * an overview diagram
 * a motivation for the decomposition
 * black box descriptions of the contained building blocks. For these we offer you alternatives:

   ** use _one_ table for a short and pragmatic overview of all contained building blocks and their interfaces
   ** use a list of black box descriptions of the building blocks according to the black box template (see below).
   Depending on your choice of tool this list could be sub-chapters (in text files), sub-pages (in a Wiki) or nested elements (in a modeling tool).


 * (optional:) important interfaces, that are not explained in the black box templates of a building block, but are very important for understanding the white box.
Since there are so many ways to specify interfaces why do not provide a specific template for them.
 In the worst case you have to specify and describe syntax, semantics, protocols, error handling,
 restrictions, versions, qualities, necessary compatibilities and many things more.
In the best case you will get away with examples or simple signatures.

////

[plantuml, target=whitebox, format=svg]
....
@startuml


component KafkaPlatform

package "core" {
  component PricingRules
  component MetricProcessor
  component ContextProvider
}

package "extensions" {
  component MetricsScraper
}


KafkaPlatform -- metricsApi

MetricsScraper -> KafkaPlatform: produce usage\ndata
MetricsScraper -> metricsApi: scrape usage\n data

MetricProcessor -> KafkaPlatform: consume\n raw metrics
MetricProcessor -> KafkaPlatform: produce\n aggregated\n metrics
MetricProcessor --> PricingRules: use
MetricProcessor -> ContextProvider: use


@enduml
....


|===
|Building block | Description

| PricingRules
| Stores rules for turning usage information into costs

| ContextProvider
a|
Manages contextual information that can be used to enrich metrics with company-specific information. E.g. relations between clientIds, applications, projects, cost centers, ...


| MetricProcessor
a|
* Defines interfaces for metrics, that must be used by _MetricsScraper_ +
* Aggregates metrics into time buckets
* Produces enriched data streams which includes contextual information

| MetricsScraper
a|
* uses a metric source, such as JMX or the confluent cloud metrics API to collect usage metrics
* transforms the collected metrics into a format that is defined by _MetricProcessor_

|===

==== MetricsScraper
===== Confluent Cloud
Confluent exposes many metrics in prometheus format.
These will be scraped with telegraf.
Some information are missing from the prometheus export endpoint and need to be fetched with custom queries/requests.
This is done with a java application which exposes them as prometheus endpoint.
Docs: https://docs.confluent.io/cloud/current/monitoring/metrics-api.html
[%autowidth]
|===
| Additional metric | endpoint/query
| Partition count of a topic
a|
`AdminClient` to list all topics, partition count and replication factor
| registered schemas for a topic
a| http requests to schema registry needed.
1. `GET {{schema_registry_url}}/schemas/`
2. Group by topic and create gauge metric for schema count by topic
WARNING:: Does not account for soft deleted topics in confluent cloud! Possible sanity check: sum of all schemas should equal total number of schemas reported by
`https://api.telemetry.confluent.cloud/v2/metrics/cloud/descriptors/metrics?resource_type=schema_registry`
If the sum is smaller, then soft deletes were made (using the max version number might yield a better result in this case).
WARNING:: If a non default naming strategy is used for subjects, then linking schemas/subjects to topics is not possible for all schemas.
a|
|===
[plantuml, target=, format=svg]
....
@startuml
'left to right direction
component Telegraf {
}
component ConfluentCloudScraper
ConfluentCloudScraper -up- prometheusEndpoint
ConfluentCloudScraper -> metricsAPI : collect additional information
component ConfluentCloud
ConfluentCloud - metricsAPI
ConfluentCloud - broker
Telegraf -> broker: push to output topic/s
Telegraf --> metricsAPI: scrape metrics
Telegraf --> prometheusEndpoint: scrape metrics
@enduml
....

==== PricingRules

[plantuml, target=pricingrules, format=svg]
....
@startuml
component PricingRules <<Kafka Streams App>> {

    [RuleEngine] --> [Metrics]: uses
    [RuleEngine] --> [RulesRepository]: reads
}
PricingRules -- "Graphql API"

queue pricing_rules <<Topic>>

[RulesRepository] --> pricing_rules: uses\n as storage
[UI] -> "Graphql API"
@enduml
....



==== ContextProvider


[plantuml, target=contextprovider, format=svg]
....
@startuml
component ContextProvider <<Kafka Streams App>> {
    [ContextRepository]
}
ContextProvider -- "Graphql API"

component Kafka {
    queue context_data <<topic>>

}

[ContextRepository] --> Kafka: uses as\n storage
[UI] -> "Graphql API"
@enduml
....


===== Context format
* metrics are defined in the core
* a metric belongs to at least one of the dimensions
** topic
** consumer group
** principal
* a context object can be attached to existing dimensions as a AVRO key-value pair to provide the needed flexibility

[source,json]
.topic context as JSON record in a topic, record key="car-claims"
----
{
  "creationTime": "2024-01-01T00:00:00Z",
  "validFrom": "2024-01-01T00:00:00Z",
  "validUntil": null,
  "entityType": "TOPIC",
  "regex": "car-claims",
  "context": {
    "project": "claims-processing",
    "organization_unit": "non-life-insurance",
    "sap_psp_element": "1234.234.abc"
  }
}
----

[source,json]
.topic context rule as JSON record in a topic, record key="default-rule-since-2020"
----
{
  "creationTime": "2024-01-01T00:00:00Z",
  "validFrom": "2024-01-01T00:00:00Z",
  "validUntil": null,
  "entityType": "TOPIC",
  "regex": "^([a-z0-9-]+)\\.([a-z0-9-]+)\\.([a-z0-9-]+)-.*$",
  "context": {
    "tenant": "$1",
    "app_id": "$2",
    "component_id": "$3"
  }
}

----

If naming conventions are very clear they could also be provided as a file / configuration.


[source,json]
.principal context as JSON record in a topic, record key="cluster-id-principal-default-ctxt"
----
{
  "creationTime": "2024-01-01T00:00:00Z",
  "validFrom": "2024-01-01T00:00:00Z",
  "validUntil": null,
  "entityType": "PRINCIPAL",
  "regex": "u-4j9my2",
  "context": {
    "project": "claims-processing",
    "organization_unit": "non-life-insurance",
    "sap_psp_element": "1234.234.abc"
  }
}
----

INFO::
Context objects will be started as AVRO messages. We use JSON as a representation here for simplicity.

===== Context Lookup

State stores in Kafka Streams will be used to construct lookup tables for the context.

The key is a string and is a free value that can be set by the user. If no key is provided the API should create random unique key. The topic is compacted, meaning if we want to delete an item we can send a null payload with its key.

.context lookup table
|===
|Key |Value

|<type>_<cluster-id>_<principal_id> |<context-object>

|PRINCIPAL_lx1dfsg_u-4j9my2_2024-01-01 |{..., "regex": "u-4j9my2","context": {...}}

|b0bd9c9a-08e6-46c7-9f71-9eafe370da6c | <context-object>
|===

Once the table has been loaded, aggregated metrics can be enriched with a KTable - Streams join.


