ifndef::imagesdir[:imagesdir: ../images]

[[section-runtime-view]]
== Runtime View

////
.Contents
The runtime view describes concrete behavior and interactions of the systemâ€™s building blocks in form of scenarios from the following areas:

* important use cases or features: how do building blocks execute them?
* interactions at critical external interfaces: how do building blocks cooperate with users and neighboring systems?
* operation and administration: launch, start-up, stop
* error and exception scenarios

Remark: The main criterion for the choice of possible scenarios (sequences, workflows) is their *architectural relevance*. It is *not* important to describe a large number of scenarios. You should rather document a representative selection.

.Motivation
You should understand how (instances of) building blocks of your system perform their job and communicate at runtime.
You will mainly capture scenarios in your documentation to communicate your architecture to stakeholders that are less willing or able to read and understand the static models (building block view, deployment view).

.Form
There are many notations for describing scenarios, e.g.

* numbered list of steps (in natural language)
* activity diagrams or flow charts
* sequence diagrams
* BPMN or EPCs (event process chains)
* state machines
* ...


.Further Information

See https://docs.arc42.org/section-6/[Runtime View] in the arc42 documentation.

////

=== Metrics Ingestion from Confluent Cloud

Process to gather and aggregate metrics from Confluent Cloud.

The Confluent Metrics Scraper calls the endpoint
`api.telemetry.confluent.cloud/v2/metrics/cloud/export?resource.kafka.id={CLUSTER-ID}`
with Basic Auth in an interval of 1 Minute to obtain all metrics in Prometheus format.

[plantuml,target=runtime-scraping,format=svg]
....
@startuml
autonumber
loop endless, every minute
    MetricsScraper -> "Confluent Cloud Metrics API": fetch metrics
    MetricsScraper <-- "Confluent Cloud Metrics API": kafka metrics in prometheus format
    MetricsScraper -> MetricsScraper: transform
    loop for each metric
        MetricsScraper -> KafkaCluster: produce metrics
        MetricsScraper <-- KafkaCluster: ack
    end
end
@enduml
....

This can be implemented with a standard tool like Telegraf.

It MUST be possible to use other sources of metrics and other scraping mechanisms.

=== Metrics Enrichment

[plantuml,target=runtime-enrich,format=svg]
....
@startuml
component "MetricsProcessor (2 aggregate)" as MetricsAggregator
component "MetricsProcessor (6 enrich)" as MetricsProcessor
queue "raw-metrics"
queue "aggregated-metrics"
queue "enriched-metrics"
database context
database "query-database"


"raw-metrics" -right-> [MetricsAggregator]: 1 consume
MetricsAggregator -right-> "aggregated-metrics" : 3 produce
"aggregated-metrics" -right-> [MetricsProcessor]: 4 consume
[MetricsProcessor] -right-> "enriched-metrics": 7 produce
[MetricsProcessor] -up-> context: 5 lookup

"aggregated-metrics" -down-> "query-database": 8 sink
"enriched-metrics" -down-> "query-database": 8 sink
@enduml
....

1. Metrics are consumed from all the raw data topics.
2. Metrics are aggregated by the MetricsAggregator.
Here we build hourly and daily aggregates.
The aggregates are made for each relevant grouping see below.
3. The aggregates are stored in the `aggregated-metrics` topic.
4. The aggregated metrics can be consumed by a customer specific metrics processor and enriched with additional information.
This can be used to add customer specific information to the metrics.
5. In order to add the customer specific information the metrics processor can use the context database or stream to look up the additional information.
6. The logic to enrich the metrics is applied
7. The enriched metrics are stored in the `enriched-metrics` topic.
8. The enriched metrics are stored into the query database. (if no enrichment is needed the aggregated metrics are stored directly into the query database)

The storage procedure into the query database must be idempotent in order to reprocess the enrichment in case of manual intervention.

==== Enrichment for topics

.metric with topic name from confluent cloud
[source,json]
----
{
  "fields": {
    "gauge": 40920
  },
  "name": "confluent_kafka_server_sent_bytes",
  "tags": {
    "env": "sdm",
    "host": "confluent.cloud",
    "kafka_id": "lkc-x5zqx",
    "topic": "mobiliar-agoora-state-global",
    "url": "https://api.telemetry.confluent.cloud/v2/metrics/cloud/export?resource.kafka.id=lkc-x5zqx"
  },
  "timestamp": 1704805140
}
----

==== Enrichment for principals

.metric with principal id from confluent cloud
[source,json]
----
{
  "fields": {
    "gauge": 0
  },
  "name": "confluent_kafka_server_request_bytes",
  "tags": {
    "env": "sdm",
    "host": "confluent.cloud",
    "kafka_id": "lkc-x5zqx",
    "principal_id": "u-4j9my2",
    "type": "ApiVersions",
    "url": "https://api.telemetry.confluent.cloud/v2/metrics/cloud/export?resource.kafka.id=lkc-x5zqx"
  },
  "timestamp": 1704805200
}
----

=== Metrics Grouping

- confluent_kafka_server_request_bytes by kafka_id (Cluster) and principal_id (User) for the type Produce as sum stored in produced_bytes
- confluent_kafka_server_response_bytes by kafka_id (Cluster) and principal_id (User) for the type Fetch as sum stored in fetched_bytes
- confluent_kafka_server_retained_bytes by kafka_id (Cluster) and topic as min and max stored in retained_bytes_min and retained_bytes_max
- confluent_kafka_server_consumer_lag_offsets by kafka_id (Cluster) and topic as list of consumer_group_id stored in consumergroups

maybe more are possible.

