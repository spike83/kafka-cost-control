ifndef::imagesdir[:imagesdir: ../images]

[[section-runtime-view]]
=== Runtime View

////
.Contents
The runtime view describes concrete behavior and interactions of the systemâ€™s building blocks in form of scenarios from the following areas:

* important use cases or features: how do building blocks execute them?
* interactions at critical external interfaces: how do building blocks cooperate with users and neighboring systems?
* operation and administration: launch, start-up, stop
* error and exception scenarios

Remark: The main criterion for the choice of possible scenarios (sequences, workflows) is their *architectural relevance*. It is *not* important to describe a large number of scenarios. You should rather document a representative selection.

.Motivation
You should understand how (instances of) building blocks of your system perform their job and communicate at runtime.
You will mainly capture scenarios in your documentation to communicate your architecture to stakeholders that are less willing or able to read and understand the static models (building block view, deployment view).

.Form
There are many notations for describing scenarios, e.g.

* numbered list of steps (in natural language)
* activity diagrams or flow charts
* sequence diagrams
* BPMN or EPCs (event process chains)
* state machines
* ...


.Further Information

See https://docs.arc42.org/section-6/[Runtime View] in the arc42 documentation.

////

==== Metrics Ingestion from Confluent Cloud

Process to gather and aggregate metrics from Confluent Cloud.

The Confluent Metrics Scraper calls the endpoint
`api.telemetry.confluent.cloud/v2/metrics/cloud/export?resource.kafka.id={CLUSTER-ID}`
with Basic Auth in an interval of 1 Minute to obtain all metrics in Prometheus format.

[plantuml,target=runtime-scraping,format=svg]
....
@startuml
autonumber
loop endless, every minute
    MetricsScraper -> "Confluent Cloud Metrics API": fetch metrics
    MetricsScraper <-- "Confluent Cloud Metrics API": kafka metrics in prometheus format
    MetricsScraper -> MetricsScraper: transform
    loop for each metric
        MetricsScraper -> KafkaCluster: produce metrics
        MetricsScraper <-- KafkaCluster: ack
    end
end
@enduml
....

Telegraf is used to poll data using Confluent prometheus endpoint.
[plantuml,target=runtime-confluent-telegraf,format=svg]
....
@startuml

cloud ConfluentMetrics
component Telegraf
queue "raw-metrics"


ConfluentMetrics <-right- Telegraf: poll prometheus endpoint
Telegraf -right-> "raw-metrics": produce

@enduml
....

==== Metrics using Kafka Admin API

Some information can be gathered from the Kafka Admin API. We will develop a simple application that connect to the Kafka Admin API and expose metrics as prometheus endpoint. We can then reuse Telegraf to publish those metrics to kafka.

[plantuml,target=runtime-kafka-admin-api,format=svg]
....
@startuml

component Kafka
component MetricAPIReader
component Telegraf
queue "raw-metrics"


Kafka <-right- MetricAPIReader: poll using admin api
MetricAPIReader <-right- Telegraf: poll prometheus endpoint
Telegraf -right-> "raw-metrics": produce

@enduml
....

==== Other sources of metrics

Anyone can publish to the raw metrics topic. The metrics should follow the telegraf format.
Recommendation: use one topic per source of metrics. The MetricEnricher application will anyway consume multiple raw metric topics.

==== Metrics Enrichment

[plantuml,target=runtime-enrich,format=svg]
....
@startuml
left to right direction

    queue "raw-metrics-1"
    queue "raw-metrics-n" #line.dashed

queue "context-data"
queue "pricing-rules-data"
queue "aggregated-metrics"
component MetricsProcessor
database "context"
database "pricing-rules"
database "query-database"


"pricing-rules-data" -down-> "pricing-rules": populate
"context-data" -down-> "context": populate

"raw-metrics-1" -down-> [MetricsProcessor]: consume
"raw-metrics-n" -down-> [MetricsProcessor]: consume
MetricsProcessor -left-> "context": lookup
MetricsProcessor -right-> "pricing-rules": lookup
MetricsProcessor -down-> "aggregated-metrics" : produce

"aggregated-metrics" -down-> "query-database": sink
@enduml
....

. Metrics are consumed from all the raw data topics.
. Metrics are aggregated by the MetricsProcessor.
Here we:
 * aggregate by hours
 * attach context
 * attach pricing rule
. The aggregates are stored in the `aggregated-metrics` topic.
. The aggregated metrics are stored into the query database.

The storage procedure into the query database must be idempotent in order to reprocess the enrichment in case of reprocessing.

===== Enrichment for topics

.metric with topic name from confluent cloud
[source,json]
----
{
  "fields": {
    "gauge": 40920
  },
  "name": "confluent_kafka_server_sent_bytes",
  "tags": {
    "env": "sdm",
    "host": "confluent.cloud",
    "kafka_id": "lkc-x5zqx",
    "topic": "mobiliar-agoora-state-global",
    "url": "https://api.telemetry.confluent.cloud/v2/metrics/cloud/export?resource.kafka.id=lkc-x5zqx"
  },
  "timestamp": 1704805140
}
----

===== Enrichment for principals

.metric with principal id from confluent cloud
[source,json]
----
{
  "fields": {
    "gauge": 0
  },
  "name": "confluent_kafka_server_request_bytes",
  "tags": {
    "env": "sdm",
    "host": "confluent.cloud",
    "kafka_id": "lkc-x5zqx",
    "principal_id": "u-4j9my2",
    "type": "ApiVersions",
    "url": "https://api.telemetry.confluent.cloud/v2/metrics/cloud/export?resource.kafka.id=lkc-x5zqx"
  },
  "timestamp": 1704805200
}
----

==== Metrics Grouping

- confluent_kafka_server_request_bytes by kafka_id (Cluster) and principal_id (User) for the type Produce as sum stored in produced_bytes
- confluent_kafka_server_response_bytes by kafka_id (Cluster) and principal_id (User) for the type Fetch as sum stored in fetched_bytes
- confluent_kafka_server_retained_bytes by kafka_id (Cluster) and topic as min and max stored in retained_bytes_min and retained_bytes_max
- confluent_kafka_server_consumer_lag_offsets by kafka_id (Cluster) and topic as list of consumer_group_id stored in consumergroups

maybe more are possible.

